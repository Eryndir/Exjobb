{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kblab, json\n",
    "import os\n",
    "import multiprocessing\n",
    "import requests\n",
    "import urllib3\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from kblab import Archive\n",
    "from kblab.utils import flerge\n",
    "from json import loads\n",
    "from tqdm import tqdm\n",
    "from urllib3.util import Retry\n",
    "from urllib3 import PoolManager, make_headers\n",
    "from itertools import product\n",
    "kblab.VERIFY_CA=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem = \"scelerisque fermentum dui faucibus in ornare quam viverra orci sagittis eu volutpat odio facilisis mauris sit amet massa vitae tortor condimentum lacinia quis vel eros donec ac odio tempor orci dapibus ultrices in iaculis nunc sed augue lacus viverra vitae congue eu consequat ac felis donec et odio pellentesque diam volutpat commodo sed egestas egestas fringilla phasellus faucibus scelerisque eleifend donec pretium vulputate sapien nec sagittis aliquam malesuada bibendum arcu vitae elementum curabitur vitae nunc sed velit dignissim sodales ut eu sem integer vitae justo eget magna fermentum iaculis eu non diam phasellus vestibulum lorem sed risus ultricies tristique nulla aliquet enim tortor at auctor urna nunc id cursus metus aliquam eleifend mi in nulla posuere sollicitudin aliquam ultrices sagittis orci a scelerisque purus semper eget duis at tellus at urna condimentum mattis pellentesque id nibh tortor id aliquet lectus proin nibh nisl condimentum id venenatis a condimentum vitae sapien pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas sed tempus urna et pharetra pharetra massa massa ultricies mi quis hendrerit dolor magna eget est lorem ipsum dolor sit amet consectetur adipiscing elit pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas integer eget aliquet nibh praesent tristique magna sit amet purus gravida quis blandit turpis cursus in hac habitasse platea dictumst quisque sagittis purus sit amet volutpat consequat mauris nunc congue nisi vitae suscipit tellus mauris a diam maecenas sed enim ut sem viverra aliquet eget sit amet tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra magna ac placerat vestibulum lectus mauris ultrices eros in cursus turpis massa tincidunt dui ut ornare lectus sit amet est placerat in egestas erat imperdiet sed euismod nisi porta lorem mollis aliquam ut porttitor leo a diam sollicitudin tempor id eu nisl nunc mi ipsum faucibus vitae aliquet nec ullamcorper sit amet risus nullam eget felis eget nunc lobortis mattis aliquam faucibus purus in massa tempor nec feugiat nisl pretium fusce id velit ut tortor pretium viverra suspendisse potenti nullam ac tortor vitae purus faucibus ornare suspendisse sed nisi lacus sed viverra tellus in hac habitasse platea dictumst vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat vivamus at augue eget arcu dictum varius duis at consectetur lorem donec massa sapien faucibus et molestie ac feugiat sed lectus vestibulum mattis ullamcorper velit sed ullamcorper morbi tincidunt ornare massa eget egestas purus viverra accumsan in nisl nisi scelerisque eu ultrices vitae auctor eu augue ut lectus arcu bibendum at varius vel pharetra vel turpis nunc eget lorem dolor sed viverra ipsum nunc aliquet bibendum enim facilisis gravida neque convallis a cras semper auctor neque vitae tempus quam pellentesque nec nam aliquam sem et tortor consequat id porta nibh venenatis cras sed felis eget velit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum integer enim neque volutpat ac tincidunt vitae semper quis lectus nulla at volutpat diam ut venenatis tellus in metus vulputate eu scelerisque felis imperdiet proin fermentum leo vel orci porta non pulvinar neque laoreet suspendisse interdum consectetur libero id faucibus nisl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_cred', 'r') as file:\n",
    "    pw = file.read().replace('\\n', '')\n",
    "\n",
    "archive = Archive(\"https://betalab.kb.se\", auth=(\"demo\", pw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(dark_id, headers):\n",
    "  http = PoolManager(cert_reqs='CERT_NONE')\n",
    "  kblab.VERIFY_CA=False\n",
    "  try:\n",
    "      print(f\"trying https://betalab.kb.se/{dark_id}/meta.json\")\n",
    "      meta_json = http.request(\n",
    "          \"GET\",\n",
    "          f\"https://betalab.kb.se/{dark_id}/meta.json\",\n",
    "          headers=headers,\n",
    "          retries=Retry(connect=5, read=4, redirect=5, backoff_factor=0.02),\n",
    "      )\n",
    "\n",
    "      meta_json = loads(meta_json.data.decode(\"utf-8\"))\n",
    "      meta_json[\"dark_id\"] = dark_id\n",
    "      return meta_json\n",
    "\n",
    "  except:\n",
    "      print(\"failed getting\")\n",
    "      return {\n",
    "          \"dark_id\": dark_id,\n",
    "          \"title\": \"failed\",\n",
    "          \"year\": \"failed\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.search('tags: \"issue\"')\n",
    "\n",
    "meta_json_list = []\n",
    "content_json_list = []\n",
    "for package_id in archive.search('tags: \"issue\"', max=2):\n",
    "    package = archive.get(package_id)\n",
    "    lmao = flerge(package)\n",
    "\n",
    "    if \"meta.json\" in package:\n",
    "        meta_json = json.load(package.get_raw(\"meta.json\"))\n",
    "        meta_json_list.append(meta_json)\n",
    "    if \"content.json\" in package:\n",
    "        content_json = json.load(package.get_raw(\"content.json\"))\n",
    "        content_json_list.append(content_json)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_json_list = content_json_list[0]\n",
    "lmao = [x[\"content\"] for x in content_json_list]\n",
    "lmao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«•» Hemb&rnin ', 'Daglig morgontidning ^']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [x[0][\"content\"] for x in content_json_list] #content_json_list[0][0][\"content\"]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'created': '1905-01-05',\n",
       "  'year': '1905',\n",
       "  'edition': '0',\n",
       "  'issue': '12474B',\n",
       "  'title': 'DAGENS NYHETER  1905-01-05'},\n",
       " {'created': '1905-02-17',\n",
       "  'year': '1905',\n",
       "  'edition': '0',\n",
       "  'issue': '12516A',\n",
       "  'title': 'DAGENS NYHETER  1905-02-17'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "meta_json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08565636  0.11254321 -0.02355101 ...  0.00929855  0.05028643\n",
      "  -0.12768164]\n",
      " [ 0.07838852  0.02995606 -0.03571041 ... -0.01521753  0.06298143\n",
      "  -0.08363984]\n",
      " [ 0.04481409  0.07148021 -0.0313642  ...  0.00074999  0.0748238\n",
      "  -0.0769266 ]\n",
      " [ 0.07923093  0.09726818 -0.04104416 ...  0.01661729  0.07949235\n",
      "  -0.15745626]\n",
      " [-0.03911571  0.08857869 -0.02402698 ...  0.06262439  0.09870593\n",
      "  -0.11599754]\n",
      " [ 0.09784506  0.05300535 -0.03907291 ...  0.06184708 -0.00861086\n",
      "  -0.10053059]]\n",
      "[ 1.32508457e-01  5.96463680e-03  3.43883149e-02  1.57257505e-02\n",
      " -1.55537473e-02 -5.31849032e-03 -2.18539521e-01  1.24811687e-01\n",
      "  8.16078633e-02  7.94360694e-03  5.81024028e-02  1.13142831e-02\n",
      "  8.09717998e-02 -4.54286374e-02  1.29294381e-01 -1.16230249e-02\n",
      "  9.54005122e-02  9.73669887e-02 -2.11207625e-02  1.14677541e-01\n",
      " -1.22725256e-01 -1.85505915e-02  5.95387109e-02  1.82689447e-02\n",
      " -1.22966431e-02  9.88732278e-02 -3.05919331e-02  5.75979203e-02\n",
      " -5.62502556e-02 -1.23067714e-01 -1.32290602e-01 -1.95828173e-02\n",
      " -9.10316780e-02 -1.27530396e-01  4.39788710e-06  1.86046109e-01\n",
      "  4.87520993e-02  9.87164378e-02 -1.68813333e-01  9.04153958e-02\n",
      " -8.24766681e-02  1.05285952e-02 -2.54686195e-02 -1.22065701e-01\n",
      "  8.27368274e-02  7.94069096e-02 -7.73115531e-02  5.64128049e-02\n",
      " -1.34441093e-01  2.18882915e-02  6.71954304e-02 -2.27016926e-01\n",
      " -8.74701422e-03  2.51680389e-02 -1.23129494e-01  1.06421553e-01\n",
      "  6.88351467e-02 -8.37971866e-02 -3.14355381e-02  1.06357627e-01\n",
      " -1.39956214e-02 -4.21089530e-02  5.17569520e-02  9.52096358e-02\n",
      "  3.05883307e-02  1.14681773e-01 -1.03317518e-02  6.43826351e-02\n",
      "  2.59409752e-02  2.94690523e-02 -8.04295316e-02  1.04798138e-01\n",
      "  6.39180169e-02  9.20126680e-03 -1.66068301e-01 -1.36019126e-01\n",
      "  1.85047194e-01 -6.25296077e-03 -8.38409290e-02 -1.40453354e-01\n",
      "  9.36150029e-02 -1.01713657e-01 -5.35160862e-03  9.84733030e-02\n",
      "  6.00406229e-02  2.24973634e-01  2.62061581e-02 -1.95609257e-02\n",
      "  1.49764404e-01 -6.27178848e-02 -2.58240779e-03  3.15900929e-02\n",
      " -2.60052993e-03  1.69932172e-01  3.31762768e-02  5.99680953e-02\n",
      "  1.35354266e-01 -4.13840339e-02  9.05435905e-03 -1.59870759e-01\n",
      " -1.59388721e-01  9.41494573e-03 -1.83924809e-01 -3.01017761e-02\n",
      "  3.84912011e-03  6.75463900e-02 -1.82987213e-01 -4.03785370e-02\n",
      " -1.59525052e-01  6.80851266e-02 -3.43163311e-02  2.51789819e-02\n",
      " -6.91126212e-02  1.82293072e-01 -8.49177421e-04 -4.33925055e-02\n",
      "  1.07522428e-01  4.38089250e-03  1.38060287e-01  2.55376454e-02\n",
      " -6.39133155e-02  5.18551730e-02 -6.18712604e-02  1.83299646e-01\n",
      " -1.98855594e-01 -1.61272977e-02 -1.07617415e-01  1.28999231e-02\n",
      "  9.56011284e-03 -4.38361755e-03 -1.26932180e-02 -1.21542281e-02\n",
      " -2.09229961e-01 -1.01186968e-01  1.62240535e-01  1.09701253e-01\n",
      " -2.91345511e-02  6.34616753e-03 -3.03482320e-02 -1.59760699e-01\n",
      " -1.24648809e-01 -2.79586408e-02 -8.79801288e-02  6.83571473e-02\n",
      "  1.27314284e-01  2.46469434e-02  9.29583535e-02  1.09211832e-01\n",
      "  6.77633509e-02  1.32204875e-01 -7.67076984e-02 -2.49962509e-02\n",
      "  1.66502550e-01  5.02295978e-02  8.80164374e-03 -5.85634671e-02\n",
      "  3.06355283e-02  8.35681986e-03  8.32769424e-02  4.34225500e-02\n",
      " -2.75852513e-02  1.86075922e-02 -1.32690743e-01 -7.32190683e-02\n",
      " -5.58730625e-02  8.62857327e-02 -6.83658272e-02  8.23629946e-02\n",
      "  8.72668326e-02 -2.39404943e-02 -2.83255521e-02  8.59283209e-02\n",
      " -9.22513381e-02 -3.01315691e-02  7.95183256e-02  2.41607968e-02\n",
      "  2.67092790e-02  8.69905725e-02 -5.53644411e-02  1.43290088e-01\n",
      "  1.44743934e-01  2.97689855e-01  5.52868880e-02  1.82855688e-03\n",
      " -1.46076560e-01 -7.42243417e-03  1.23077877e-01 -1.01720661e-01\n",
      " -5.42586707e-02  3.14512812e-02  1.70634136e-01 -8.50890800e-02\n",
      "  1.53496683e-01 -1.06929980e-01  1.56643271e-01  3.12346872e-02\n",
      " -1.13479026e-01 -3.73145267e-02  1.97425019e-02 -5.84979765e-02\n",
      "  1.28765509e-01  8.45585689e-02  1.72016546e-01 -5.22137322e-02\n",
      "  3.86152826e-02 -7.44012371e-03 -1.93230230e-02 -1.76092330e-02\n",
      " -3.68712582e-02 -1.28309652e-01 -9.74066928e-02 -4.28703688e-02\n",
      " -3.16300802e-02 -5.69678210e-02 -1.32637426e-01  4.54912148e-02\n",
      "  5.67335337e-02  1.73636468e-03  1.14048839e-01  8.11516419e-02\n",
      " -7.84638003e-02  1.03062458e-01  1.88589692e-01  9.87514779e-02\n",
      " -3.97559293e-02 -1.02831550e-01 -2.34487280e-02 -7.23284408e-02\n",
      " -9.70926657e-02 -1.89714786e-02  1.04067497e-01  3.88219878e-02\n",
      " -9.88263488e-02  2.04197075e-02 -7.46075138e-02  5.78819402e-02\n",
      " -1.06929332e-01 -1.08561786e-02  1.77289394e-03  2.65639257e-02\n",
      " -1.27812251e-01 -2.02119630e-02 -2.68324465e-02 -8.59586000e-02\n",
      " -5.07235043e-02 -1.72973588e-01 -8.44971556e-03 -1.08190097e-01\n",
      " -1.55470535e-01  8.83646086e-02 -1.51005574e-04 -8.78978893e-03\n",
      "  4.67425287e-02  3.79145443e-02  1.47600248e-01 -7.17720166e-02\n",
      " -3.40502374e-02  1.35754034e-01 -1.08472509e-02  1.85169265e-01\n",
      " -8.59157443e-02 -2.02509109e-02  4.97871675e-02 -1.72091685e-02\n",
      "  1.57079920e-01 -5.05980216e-02 -4.88017313e-02  1.55963106e-02\n",
      "  1.46198556e-01 -1.43249646e-01 -1.08444393e-02  1.43701434e-01\n",
      "  6.54840544e-02  4.33071405e-02  2.56923139e-02 -2.26631612e-02\n",
      " -1.42881395e-02 -1.08782023e-01  1.97109953e-02 -4.46943343e-02\n",
      "  9.43932012e-02  2.69056559e-01  3.69449370e-02  5.32365330e-02\n",
      "  1.18983490e-02  1.13613449e-01 -2.63674140e-01 -2.22485557e-01\n",
      "  7.71341696e-02 -2.00967133e-01 -9.48642101e-03  3.09171784e-03\n",
      " -2.70780493e-02 -1.09195970e-02  1.07676797e-01  1.08375408e-01\n",
      " -5.81493415e-02  3.90948504e-02  9.50257555e-02  1.73013024e-02\n",
      "  2.63115540e-02 -5.76742589e-02  6.95649907e-02  1.61204748e-02\n",
      " -2.63932031e-02  1.82842508e-01  1.00283893e-02  1.87526401e-02\n",
      "  7.60981739e-02  2.97264103e-02  1.21253520e-01  1.11932434e-01\n",
      " -4.64130081e-02 -1.07275553e-01 -9.25462842e-02 -1.58827245e-01\n",
      " -7.31873065e-02  1.19385518e-01 -1.92453072e-03 -1.80087134e-01\n",
      "  4.38299356e-03  4.63893563e-02  9.00205150e-02 -5.38607128e-02\n",
      "  4.12706174e-02  6.36663958e-02  5.53373210e-02 -1.31966442e-01\n",
      " -1.80029180e-02  5.90767292e-03  1.60360914e-02 -1.10372743e-02\n",
      " -3.04859187e-02 -1.47632673e-01  1.62559018e-01  2.24665254e-02\n",
      " -1.13152198e-01  5.93865477e-02 -1.68499187e-01 -1.19468354e-01\n",
      "  2.82210987e-02  6.60067424e-02 -6.52193651e-02 -5.12248687e-02\n",
      " -1.26491770e-01  1.17697768e-01  1.71001673e-01 -3.92336287e-02\n",
      " -2.14102328e-01 -1.25229195e-01 -1.26149923e-01 -2.14390054e-01\n",
      "  1.67360194e-02 -9.28607285e-02 -7.35382363e-02  7.08490089e-02\n",
      "  2.60167792e-02  1.21299379e-01 -5.56614995e-02  1.82366177e-01\n",
      "  1.55818276e-03  9.62147489e-02  4.87927012e-02  6.79482892e-02\n",
      "  9.29813683e-02  1.70165911e-01  7.17420503e-02 -6.38485595e-04\n",
      "  1.79385781e-01 -2.62484644e-02 -1.53020434e-02  1.04640521e-01\n",
      "  2.37083789e-02 -4.78631966e-02  4.25898284e-02  3.31276166e-03\n",
      "  6.95562735e-02  6.10197671e-02 -1.09529853e-01 -3.52106206e-02\n",
      "  1.53029755e-01 -1.66419923e-01 -4.46834695e-03  1.83463562e-02\n",
      " -7.21515715e-02 -8.99953246e-02  3.30642015e-02 -1.75436020e-01\n",
      "  2.13761069e-03 -3.37699354e-02  7.83217102e-02 -1.93224885e-02\n",
      " -3.62358056e-02 -1.38488099e-01 -3.90810845e-03  3.09589505e-02\n",
      " -4.55209725e-02  6.04710467e-02  9.11211744e-02  9.56897140e-02\n",
      "  1.99945167e-01  1.28010556e-01  9.54732001e-02 -6.17960729e-02\n",
      "  2.56873365e-03 -1.47847608e-02 -1.84192769e-02  8.22721496e-02\n",
      "  2.75327209e-02  8.66548643e-02  6.44554943e-02  1.90806370e-02\n",
      " -1.11120082e-01 -6.72703162e-02  1.89831436e-01 -1.52691454e-01\n",
      " -1.52131366e-02 -2.41045788e-01  1.42901689e-02 -1.24987133e-01\n",
      " -7.25695267e-02  4.94695641e-02  7.60163888e-02  1.09813482e-01\n",
      "  1.03781931e-04 -1.83791369e-02 -9.44222137e-02  8.46970975e-02\n",
      " -4.60073240e-02 -1.43649384e-01  1.15702868e-01 -1.87829062e-02\n",
      " -4.07294184e-02 -2.76194125e-01  8.82170200e-02 -2.21328493e-02\n",
      "  2.65842229e-01 -5.04225604e-02 -6.19782694e-02  2.47403998e-02\n",
      "  1.17600612e-01 -7.51456693e-02 -1.00499682e-01 -5.17788939e-02\n",
      "  8.07919726e-02  1.06639504e-01  2.90184561e-02  7.04869926e-02\n",
      " -1.51718380e-02  1.08193755e-02  4.76791747e-02 -9.88516435e-02\n",
      " -1.33116290e-01  1.25289708e-01 -1.48022562e-01  5.05766310e-02\n",
      "  1.81038737e-01 -1.04423992e-01 -1.97483394e-02 -9.51625779e-02\n",
      "  2.04223096e-02 -1.97742395e-02  4.31901127e-01 -5.42708077e-02\n",
      " -7.16589466e-02 -9.25946310e-02 -1.44297302e-01 -4.04517539e-02\n",
      " -8.37656558e-02 -7.25901639e-03 -8.64887610e-02  1.90712754e-02\n",
      " -6.71625808e-02 -6.61693094e-03 -7.17914999e-02 -2.13715956e-02\n",
      " -5.70771880e-02  8.75708275e-03 -7.20122531e-02  7.72618577e-02\n",
      " -1.52074277e-01 -1.26094535e-01  3.71927209e-02 -5.36628775e-02\n",
      "  2.41257828e-02 -8.40243697e-02  8.56582001e-02  4.86503914e-03\n",
      " -2.94515286e-02 -7.11354613e-02  1.16113193e-01  6.92082047e-02\n",
      " -1.68408647e-01 -2.51147121e-01 -9.70330909e-02  7.42632896e-02\n",
      " -2.00690627e-02 -1.37921154e-01  6.71274364e-02 -9.91099179e-02\n",
      " -3.59731093e-02 -1.40989022e-02  3.31894644e-02 -1.17904702e-02\n",
      " -1.27433717e-01 -9.33318119e-03 -6.65985718e-02 -3.82354204e-03\n",
      "  5.40680327e-02 -2.85636988e-02  1.50901407e-01 -8.52297619e-02\n",
      " -7.30766878e-02 -7.36000622e-03 -1.85940396e-02 -2.58312792e-01\n",
      " -1.36656180e-01 -1.74355105e-01 -1.16954833e-01 -3.87403555e-02\n",
      "  1.13148160e-01 -2.86161993e-02  8.14256072e-02 -6.32182732e-02\n",
      " -1.65851414e-02  1.02828972e-01 -5.80488406e-02 -2.05881777e-03\n",
      "  4.55145352e-03  1.34926543e-01 -1.22476339e-01 -5.78392446e-02\n",
      "  1.44043371e-01 -5.02371276e-03  1.21835060e-01 -5.91953611e-03\n",
      " -1.03122927e-01 -5.64031117e-02 -6.33782074e-02  1.99201647e-02\n",
      "  3.97329889e-02  1.92274023e-02 -1.05302490e-01 -1.70294649e-03\n",
      "  1.75347492e-01  4.73166518e-02 -9.68248025e-03  5.60421795e-02\n",
      " -2.01728456e-02 -1.74190223e-01  2.69410592e-02  2.58891881e-01\n",
      " -1.44917741e-01 -1.31631391e-02  1.82761565e-01 -1.58859260e-32\n",
      "  8.59818533e-02 -1.24939047e-01  6.87704533e-02  1.97753552e-02\n",
      "  4.31982428e-02 -4.50274907e-02  5.74507527e-02 -7.54321739e-02\n",
      "  1.81926385e-01 -5.07702641e-02 -4.91471142e-02 -3.63557786e-02\n",
      "  4.10111509e-02  3.28978919e-03 -7.16441125e-02 -1.14272654e-01\n",
      "  5.36476411e-02 -8.69641732e-03  7.43336678e-02  1.87662486e-02\n",
      " -2.19584405e-01  1.76574346e-02 -2.83550527e-02  2.40962803e-02\n",
      "  6.85380474e-02 -1.00697912e-01  2.28189483e-01 -5.14075421e-02\n",
      " -1.06467515e-01  4.78795217e-03 -1.04211152e-01  4.19454835e-02\n",
      "  8.59065652e-02  1.15811370e-01  2.50702277e-02  2.97696758e-02\n",
      " -6.62819743e-02  6.59061745e-02 -5.73460907e-02  8.43346342e-02\n",
      " -1.91959083e-01  4.98660952e-02 -7.89680853e-02  4.27023619e-02\n",
      "  4.11828049e-02  1.39671519e-01  2.06911582e-02  6.77568465e-02\n",
      " -9.47595835e-02  6.77220672e-02  8.03125277e-02  6.59251660e-02\n",
      "  8.09603184e-02 -1.32036120e-01 -3.67207751e-02  2.78039217e-01\n",
      "  5.68470508e-02 -3.57647315e-02  1.49257764e-01 -2.45749634e-02\n",
      "  8.22766945e-02 -7.80657679e-02 -5.29772341e-02  2.00014353e-01\n",
      " -5.01511060e-02 -1.26084425e-02  2.32265994e-01  1.84147537e-01\n",
      " -1.33247808e-01 -4.41251248e-02 -9.77509990e-02 -4.97903526e-02\n",
      "  6.96587265e-02 -2.37139329e-01 -2.08194837e-01 -2.32224584e-01\n",
      "  8.15029815e-02  4.14610058e-02  3.08397487e-02  6.95859641e-03\n",
      " -8.54675472e-02 -9.13192108e-02  1.71795115e-01 -5.28988875e-02\n",
      " -9.05638412e-02  3.36432829e-02 -3.72153311e-03  2.70876121e-02\n",
      " -2.35886648e-02 -7.23793432e-02  6.04367256e-02 -1.22273751e-01\n",
      "  2.94874739e-02  2.11850941e-01 -9.35578812e-03 -3.39648537e-02\n",
      " -5.64185716e-02 -1.19189583e-01  3.62063237e-02  4.35120948e-02\n",
      " -1.30477697e-01 -7.47243762e-02  1.49817005e-01  3.51790525e-02\n",
      " -3.63976210e-02 -6.91542998e-02  2.60128453e-02  1.40617713e-01\n",
      "  8.37099254e-02 -1.62015364e-01 -9.62965265e-02  1.80052649e-02\n",
      "  1.80847108e-01  9.15912464e-02  3.27304192e-02  1.52556002e-01\n",
      " -8.66113305e-02  4.13687481e-03 -1.38156161e-01 -3.27234045e-02\n",
      " -1.60141736e-01 -6.70005828e-02 -5.21507813e-03 -3.06833182e-02\n",
      "  1.41258060e-03 -8.58497713e-03 -2.66176965e-02  5.54939620e-02\n",
      " -6.32915571e-02 -8.90071914e-02  1.17050029e-01 -1.08453639e-01\n",
      "  6.49952199e-07  8.47451985e-02  3.67374085e-02 -1.10600283e-02\n",
      " -2.23331705e-01 -1.40894964e-01 -1.24278329e-01  1.19622676e-02\n",
      "  6.90738261e-02 -1.01582408e-01 -2.72884760e-02  1.31252304e-01\n",
      " -8.58353004e-02 -1.12271614e-01 -2.20330264e-02  1.74978375e-01\n",
      " -3.23823392e-02 -8.07094947e-02 -5.76012582e-03  4.13203165e-02\n",
      "  6.87114196e-03  6.62180632e-02 -1.12407379e-01 -1.04397774e-01\n",
      "  5.00341412e-03 -1.21439844e-02  1.83850646e-01  4.71869223e-02\n",
      " -9.60470811e-02 -1.79147646e-01 -1.02829911e-01  7.59871379e-02\n",
      "  1.70491233e-01  1.53149739e-01 -4.17888723e-02 -4.70034778e-02\n",
      " -8.12346712e-02  6.03078008e-02 -1.05098732e-01 -3.47960070e-02\n",
      " -5.92946559e-02 -1.35117739e-01 -6.30144998e-02 -8.54071081e-02\n",
      "  8.94801915e-02 -1.49051771e-01 -1.11704074e-01  8.11833516e-02\n",
      "  2.32849479e-01  8.81132483e-02  1.73385348e-03 -1.29762754e-01\n",
      "  4.34741117e-02  1.80914208e-01  1.14228792e-01 -2.40228307e-02\n",
      "  3.38517018e-02  1.50591746e-01 -1.20830908e-02  3.16685401e-02\n",
      "  6.55640215e-02  1.45893646e-02  4.60476428e-03 -6.54980466e-02\n",
      " -2.36871362e-01 -1.15823634e-01  1.45388423e-02 -4.43036146e-02\n",
      "  3.99015726e-34 -7.61584565e-02  6.27911612e-02  9.68888998e-02\n",
      " -3.88892442e-02 -7.62112737e-02 -1.65423173e-02 -5.31926788e-02\n",
      "  9.37263668e-02  6.62795752e-02  3.30970548e-02 -7.44911730e-02]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.\", \n",
    "\"Varje exempel blir konverteradHan förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.\",\n",
    "\"Det var ett sunkigt hak med ganska gott käk.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.\",\n",
    "\"Han inmundigade middagen tillsammans med ett glas rödvin.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.\",\n",
    "\"Potatischips är jättegoda.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.\",\n",
    "\"Tryck på knappen för att få tala med kundsupporten.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.Han förtärde en närande och nyttig måltid.\"]\n",
    "\n",
    "source = \"Mannen åt mat.\"\n",
    "\n",
    "model = SentenceTransformer('KBLab/sentence-bert-swedish-cased')\n",
    "embeddings = model.encode(sentences)\n",
    "sourceEmb = model.encode(source)\n",
    "print(embeddings)\n",
    "print(sourceEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "hits = semantic_search(sourceEmb, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 0, 'score': 0.7916003465652466},\n",
       "  {'corpus_id': 3, 'score': 0.5491536855697632},\n",
       "  {'corpus_id': 2, 'score': 0.323931485414505},\n",
       "  {'corpus_id': 4, 'score': 0.27011963725090027},\n",
       "  {'corpus_id': 5, 'score': 0.007242000196129084},\n",
       "  {'corpus_id': 1, 'score': -0.058579858392477036}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_20newsgroups\n\u001b[1;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mremove\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheaders\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfooters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquotes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:269\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    268\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m     cache \u001b[38;5;241m=\u001b[39m \u001b[43m_download_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwenty_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_path\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20Newsgroups dataset not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:82\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     78\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(archive_path)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Store a zipped pickle\u001b[39;00m\n\u001b[0;32m     81\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m---> 82\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[43mload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     83\u001b[0m     test\u001b[38;5;241m=\u001b[39mload_files(test_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     85\u001b[0m compressed_content \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mencode(pickle\u001b[38;5;241m.\u001b[39mdumps(cache), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzlib_codec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\sklearn\\datasets\\_base.py:257\u001b[0m, in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state, allowed_extensions)\u001b[0m\n\u001b[0;32m    255\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames:\n\u001b[1;32m--> 257\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     data \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mdecode(encoding, decode_error) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\pathlib.py:1126\u001b[0m, in \u001b[0;36mPath.read_bytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;124;03m    Open the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      2\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic(embedding_model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m----> 4\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\bertopic\\_bertopic.py:387\u001b[0m, in \u001b[0;36mBERTopic.fit_transform\u001b[1;34m(self, documents, embeddings, images, y)\u001b[0m\n\u001b[0;32m    384\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Transforming documents to embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m select_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model,\n\u001b[0;32m    386\u001b[0m                                           language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m--> 387\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding - Completed \u001b[39m\u001b[38;5;130;01m\\u2713\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\bertopic\\_bertopic.py:3410\u001b[0m, in \u001b[0;36mBERTopic._extract_embeddings\u001b[1;34m(self, documents, images, method, verbose)\u001b[0m\n\u001b[0;32m   3408\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39membed_words(words\u001b[38;5;241m=\u001b[39mdocuments, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m   3409\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 3410\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m documents[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to use an embedding model that can either embed documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3413\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor images depending on which you want to embed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\bertopic\\backend\\_base.py:69\u001b[0m, in \u001b[0;36mBaseEmbedder.embed_documents\u001b[1;34m(self, document, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m                     document: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m     57\u001b[0m                     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\" Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    matrix of embeddings\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py:65\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     52\u001b[0m           documents: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m     53\u001b[0m           verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124;03m\"\"\" Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    matrix of embeddings\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:517\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    514\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 517\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    519\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    685\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m         output_attentions,\n\u001b[0;32m    692\u001b[0m     )\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    574\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    583\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    506\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    524\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:286\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    284\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    287\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    289\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(embedding_model=model)\n",
    "\n",
    "topics, probs = topic_model.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
